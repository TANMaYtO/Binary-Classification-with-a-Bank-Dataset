{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"},{"sourceId":9293783,"sourceType":"datasetVersion","datasetId":5626665},{"sourceId":257750413,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":3517.088889,"end_time":"2025-08-24T17:24:58.698779","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-24T16:26:21.60989","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train More - XGB + NN - Achieve LB Boost!\nPreviously when we ensembled our 7 KFold XGB with our 7 KFold NN, we achieved LB 0.97730 (see [here][1]). \n\nWe will now attempt to boost their CV and LB! Previously our XGB learning rate was 0.1 for experiments. We will now train it with 0.02 for our final submission. Previously our XGB had 7 folds, we will now train it with 10 folds.\n\nPreviously our NN had 7 folds, we will now train it with 10 folds. And we will train the same 10 folds 5x times. Each time we will use a different seed.\n\nAfterward, we will ensemble all these new XGB and all these new NN. We will try to beat CV 0.97630 and LB 0.97730 by training more!\n\n==============================\n\n**NOTE** Version 1 and 2 of this notebook have a bug where the OOF and PREDS of the multiple NN are not being saved correctly. Inside each repeat for-loop, the variable OOF and PREDS was being reset to zero. \n\n[1]: https://www.kaggle.com/code/adilshamim8/bank-term-deposit-prediction\n[2]: https://www.kaggle.com/code/cdeotte/xgboost-using-original-data-cv-0-976\n[3]: https://www.kaggle.com/code/cdeotte/nn-by-gpt5-cv-0-974-wow","metadata":{"papermill":{"duration":0.006713,"end_time":"2025-08-24T16:26:25.961966","exception":false,"start_time":"2025-08-24T16:26:25.955253","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data\nWe load train, test, and original datasets. In every Kaggle playground competition, the data is synthetic and it is generated from an original dataset. In this competition, the original dataset is [here][1]\n\n[1]: https://www.kaggle.com/datasets/sushant097/bank-marketing-dataset-full","metadata":{"papermill":{"duration":0.005696,"end_time":"2025-08-24T16:26:25.973545","exception":false,"start_time":"2025-08-24T16:26:25.967849","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd, numpy as np, os\nimport cudf\n\nPATH = \"/kaggle/input/playground-series-s5e8/\"\ntrain = cudf.read_csv(f\"{PATH}train.csv\").set_index('id')\nprint(\"Train shape\", train.shape )\ntrain.head()","metadata":{"papermill":{"duration":9.918365,"end_time":"2025-08-24T16:26:35.897635","exception":false,"start_time":"2025-08-24T16:26:25.97927","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T06:00:37.176903Z","iopub.execute_input":"2025-08-31T06:00:37.177175Z","iopub.status.idle":"2025-08-31T06:00:39.813233Z","shell.execute_reply.started":"2025-08-31T06:00:37.177154Z","shell.execute_reply":"2025-08-31T06:00:39.812492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = cudf.read_csv(f\"{PATH}test.csv\").set_index('id')\ntest['y'] = np.random.randint(0, 2, len(test))\nprint(\"Test shape\", test.shape )\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:41.121656Z","iopub.execute_input":"2025-08-31T06:00:41.122111Z","iopub.status.idle":"2025-08-31T06:00:41.201474Z","shell.execute_reply.started":"2025-08-31T06:00:41.122086Z","shell.execute_reply":"2025-08-31T06:00:41.200741Z"},"papermill":{"duration":0.297657,"end_time":"2025-08-24T16:26:36.203689","exception":false,"start_time":"2025-08-24T16:26:35.906032","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig = cudf.read_csv(\"/kaggle/input/bank-marketing-dataset-full/bank-full.csv\",delimiter=\";\")\norig['y'] = orig.y.map({'yes':1,'no':0})\norig['id'] = (np.arange(len(orig))+1e6).astype('int')\norig = orig.set_index('id')\nprint(\"Original data shape\", orig.shape )\norig.head()","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:41.362937Z","iopub.execute_input":"2025-08-31T06:00:41.363151Z","iopub.status.idle":"2025-08-31T06:00:41.574955Z","shell.execute_reply.started":"2025-08-31T06:00:41.363136Z","shell.execute_reply":"2025-08-31T06:00:41.574149Z"},"papermill":{"duration":0.33052,"end_time":"2025-08-24T16:26:36.541667","exception":false,"start_time":"2025-08-24T16:26:36.211147","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)\nWe now combine all data together and then explore the columns and their properties. We observe that there is no missing data. And we observe that the categorical columns have low cardinality (i.e. under 12). We observe that most numerical columns have few unique values, and two numerical columns have around 2k and 8k unique values.","metadata":{"papermill":{"duration":0.00656,"end_time":"2025-08-24T16:26:36.556261","exception":false,"start_time":"2025-08-24T16:26:36.549701","status":"completed"},"tags":[]}},{"cell_type":"code","source":"combine = cudf.concat([train,test,orig],axis=0)\nprint(\"Combined data shape\", combine.shape )","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:41.761355Z","iopub.execute_input":"2025-08-31T06:00:41.761991Z","iopub.status.idle":"2025-08-31T06:00:41.777698Z","shell.execute_reply.started":"2025-08-31T06:00:41.761970Z","shell.execute_reply":"2025-08-31T06:00:41.777097Z"},"papermill":{"duration":0.026057,"end_time":"2025-08-24T16:26:36.588757","exception":false,"start_time":"2025-08-24T16:26:36.5627","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CATS = []\nNUMS = []\nfor c in combine.columns[:-1]:\n    t = \"CAT\"\n    if combine[c].dtype=='object':\n        CATS.append(c)\n    else:\n        NUMS.append(c)\n        t = \"NUM\"\n    n = combine[c].nunique()\n    na = combine[c].isna().sum()\n    print(f\"[{t}] {c} has {n} unique and {na} NA\")\nprint(\"CATS:\", CATS )\nprint(\"NUMS:\", NUMS )","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:41.911798Z","iopub.execute_input":"2025-08-31T06:00:41.911999Z","iopub.status.idle":"2025-08-31T06:00:41.972057Z","shell.execute_reply.started":"2025-08-31T06:00:41.911984Z","shell.execute_reply":"2025-08-31T06:00:41.971491Z"},"papermill":{"duration":0.107619,"end_time":"2025-08-24T16:26:36.703317","exception":false,"start_time":"2025-08-24T16:26:36.595698","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (LE - Label Encode)\nWe will label encode all categorical columns. Also we will make a duplicate of each numerical column and treat the copy as a categorical column.","metadata":{"papermill":{"duration":0.006723,"end_time":"2025-08-24T16:26:36.716806","exception":false,"start_time":"2025-08-24T16:26:36.710083","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CATS1 = []\nSIZES = {}\nfor c in NUMS + CATS:\n    n = c\n    if c in NUMS: \n        n = f\"{c}2\"\n        CATS1.append(n)\n    combine[n],_ = combine[c].factorize()\n    SIZES[n] = combine[n].max()+1\n\n    combine[c] = combine[c].astype('int32')\n    combine[n] = combine[n].astype('int32')\n\nprint(\"New CATS:\", CATS1 )\nprint(\"Cardinality of all CATS:\", SIZES )","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:47.911627Z","iopub.execute_input":"2025-08-31T06:00:47.911881Z","iopub.status.idle":"2025-08-31T06:00:48.155397Z","shell.execute_reply.started":"2025-08-31T06:00:47.911865Z","shell.execute_reply":"2025-08-31T06:00:48.154820Z"},"papermill":{"duration":0.35291,"end_time":"2025-08-24T16:26:37.076424","exception":false,"start_time":"2025-08-24T16:26:36.723514","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (Combine Column Pairs)\nWe will create a new categorical column from every pair of existing categorical columns. The original categorical columns have been label encoded into integers from `0 to N-1` each. Therefore we can create a new column with unique integers using the formula `new_cols[name] = combine[c1] * SIZES[c2] + combine[c2]`.","metadata":{"papermill":{"duration":0.006671,"end_time":"2025-08-24T16:26:37.090358","exception":false,"start_time":"2025-08-24T16:26:37.083687","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from itertools import combinations\n\npairs = combinations(CATS + CATS1, 2)\nnew_cols = {}\nCATS2 = []\n\nfor c1, c2 in pairs:\n    name = \"_\".join(sorted((c1, c2)))\n    new_cols[name] = combine[c1] * SIZES[c2] + combine[c2]\n    CATS2.append(name)\nif new_cols:\n    new_df = cudf.DataFrame(new_cols)         \n    combine = cudf.concat([combine, new_df], axis=1) \n\nprint(f\"Created {len(CATS2)} new CAT columns\")","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:50.396740Z","iopub.execute_input":"2025-08-31T06:00:50.397317Z","iopub.status.idle":"2025-08-31T06:00:50.614277Z","shell.execute_reply.started":"2025-08-31T06:00:50.397294Z","shell.execute_reply":"2025-08-31T06:00:50.613506Z"},"papermill":{"duration":0.275914,"end_time":"2025-08-24T16:26:37.372611","exception":false,"start_time":"2025-08-24T16:26:37.096697","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (CE - Count Encoding)\nWe now have 136 categorical columns. We will count encode each of them and create 136 new columns.","metadata":{"papermill":{"duration":0.006911,"end_time":"2025-08-24T16:26:37.386759","exception":false,"start_time":"2025-08-24T16:26:37.379848","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CE = []\nCC = CATS+CATS1+CATS2\ncombine['i'] = np.arange( len(combine) )\n\nprint(f\"Processing {len(CC)} columns... \",end=\"\")\nfor i,c in enumerate(CC):\n    if i%10==0: print(f\"{i}, \",end=\"\")\n    tmp = combine.groupby(c).y.count()\n    tmp = tmp.astype('int32')\n    tmp.name = f\"CE_{c}\"\n    CE.append( f\"CE_{c}\" )\n    combine = combine.merge(tmp, on=c, how='left')\ncombine = combine.sort_values('i')\nprint()","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:00:54.047120Z","iopub.execute_input":"2025-08-31T06:00:54.047783Z","iopub.status.idle":"2025-08-31T06:01:05.045530Z","shell.execute_reply.started":"2025-08-31T06:00:54.047753Z","shell.execute_reply":"2025-08-31T06:01:05.044884Z"},"papermill":{"duration":11.520027,"end_time":"2025-08-24T16:26:48.913643","exception":false,"start_time":"2025-08-24T16:26:37.393616","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = combine.iloc[:len(train)]\ntest = combine.iloc[len(train):len(train)+len(test)]\norig = combine.iloc[-len(orig):]\ndel combine\nprint(\"Train shape\", train.shape,\"Test shape\", test.shape,\"Original shape\", orig.shape )","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:01:05.046750Z","iopub.execute_input":"2025-08-31T06:01:05.046974Z","iopub.status.idle":"2025-08-31T06:01:05.072467Z","shell.execute_reply.started":"2025-08-31T06:01:05.046949Z","shell.execute_reply":"2025-08-31T06:01:05.071931Z"},"papermill":{"duration":0.036197,"end_time":"2025-08-24T16:26:48.957486","exception":false,"start_time":"2025-08-24T16:26:48.921289","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering (TE - Original Data as Cols)\nBelow is a technique to add the original data as new columns.","metadata":{"papermill":{"duration":0.007084,"end_time":"2025-08-24T16:26:48.97202","exception":false,"start_time":"2025-08-24T16:26:48.964936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"TE = []\nCC = CATS+CATS1+CATS2\n\n#mn = orig.y.mean() # WE FILL NAN AFTER XGB\nprint(f\"Processing {len(CC)} columns... \",end=\"\")\nfor i,c in enumerate(CC):\n    if i%10==0: print(f\"{i}, \",end=\"\")\n    tmp = orig.groupby(c).y.mean()\n    tmp = tmp.astype('float32')\n    NAME = f\"TE_ORIG_{c}\"\n    tmp.name = NAME\n    TE.append( NAME )\n    train = train.merge(tmp, on=c, how='left')\n    #train[NAME] = train[NAME].fillna(mn) # WE FILL NAN AFTER XGB\n    test = test.merge(tmp, on=c, how='left')\n    #test[NAME] = test[NAME].fillna(mn) # WE FILL NAN AFTER XGB\ntrain = train.sort_values('i')\ntest = test.sort_values('i')\nprint()","metadata":{"execution":{"iopub.status.busy":"2025-08-31T06:01:05.073072Z","iopub.execute_input":"2025-08-31T06:01:05.073257Z","iopub.status.idle":"2025-08-31T06:01:30.820692Z","shell.execute_reply.started":"2025-08-31T06:01:05.073243Z","shell.execute_reply":"2025-08-31T06:01:30.819898Z"},"papermill":{"duration":27.916044,"end_time":"2025-08-24T16:27:16.894753","exception":false,"start_time":"2025-08-24T16:26:48.978709","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train More - Train XGB w/ Original Data as Rows\nPreviously our XGB learning rate was 0.1 for experiments. We will now train it with 0.02 for our final submission. Previously our XGB had 7 folds, we will now train it with 10 folds.","metadata":{"papermill":{"duration":0.008124,"end_time":"2025-08-24T16:27:16.917176","exception":false,"start_time":"2025-08-24T16:27:16.909052","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from cuml.preprocessing import TargetEncoder\nfrom sklearn.model_selection import StratifiedKFold\nFOLDS = 5\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nimport xgboost as xgb\n\nprint(f\"XGBoost version {xgb.__version__}\")\n\nFEATURES = NUMS+CATS+CATS1+CATS2+CE\nprint(f\"We have {len(FEATURES)} features.\")\n\nFOLDS = 10\nSEED = 42\n\nparams_A = {\n    \"objective\":\"binary:logistic\",\"eval_metric\":\"auc\",\n    \"tree_method\":\"gpu_hist\",\"device\":\"cuda\",\n    \"learning_rate\":0.03, \"max_depth\":0,\n    \"grow_policy\":\"lossguide\",\"max_leaves\":64,\n    \"min_child_weight\":8,\n    \"subsample\":0.8,\"colsample_bytree\":0.6,\"colsample_bynode\":0.8,\n    \"reg_alpha\":2.0,\"reg_lambda\":12.0,\n    \"seed\":SEED\n}\n\nparams_B = {\n    \"objective\":\"binary:logistic\",\"eval_metric\":\"auc\",\n    \"tree_method\":\"gpu_hist\",\"device\":\"cuda\",\n    \"learning_rate\":0.02, \"max_depth\":0,\n    \"grow_policy\":\"lossguide\",\"max_leaves\":96,\n    \"min_child_weight\":4,\n    \"subsample\":0.7,\"colsample_bytree\":0.9,\"colsample_bynode\":0.8,\n    \"reg_alpha\":1.0,\"reg_lambda\":6.0,\n    \"seed\":SEED+13\n}\n\n\nclass IterLoadForDMatrix(xgb.core.DataIter):\n    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n        self.features = features\n        self.target = target\n        self.df = df\n        self.it = 0 \n        self.batch_size = batch_size\n        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n        super().__init__()\n\n    def reset(self):\n        '''Reset the iterator'''\n        self.it = 0\n\n    def next(self, input_data):\n        '''Yield next batch of data.'''\n        if self.it == self.batches:\n            return 0 # Return 0 when there's no more batch.\n        \n        a = self.it * self.batch_size\n        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n        #dt = cudf.DataFrame(self.df.iloc[a:b])\n        dt = self.df.iloc[a:b]\n        input_data(data=dt[self.features], label=dt[self.target]) \n        self.it += 1\n        return 1\n\noof_preds = np.zeros(len(train))\ntest_preds = np.zeros(len(test))\n\nREPEATS = 3\nfor kk in range(REPEATS):\n    for fold, (train_idx, val_idx) in enumerate(skf.split(train, train['y'])):\n        print(\"#\"*25)\n        print(f\"### REPEAT {kk+1}, Fold {fold+1} ###\")\n        print(\"#\"*25)\n    \n        Xy_train = train.iloc[train_idx][ FEATURES+['y'] ].copy()\n        Xy_more = orig[ FEATURES+['y'] ]\n        for k in range(1):\n            Xy_train = cudf.concat([Xy_train,Xy_more],axis=0,ignore_index=True)\n        \n        X_valid = train.iloc[val_idx][FEATURES].copy()\n        y_valid = train.iloc[val_idx]['y']\n        X_test = test[FEATURES].copy()\n    \n        CC = CATS1+CATS2\n        print(f\"Target encoding {len(CC)} features... \",end=\"\")\n        for i,c in enumerate(CC):\n            if i%10==0: print(f\"{i}, \",end=\"\")\n            TE0 = TargetEncoder(n_folds=10,smoothing=20, split_method='random',stat='mean',noise=0.01)\n            Xy_train[c] = TE0.fit_transform(Xy_train[c],Xy_train['y']).astype('float32')\n            X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n            X_test[c] = TE0.transform(X_test[c]).astype('float32')\n        print()\n    \n        Xy_train[CATS] = Xy_train[CATS].astype('category')\n        X_valid[CATS] = X_valid[CATS].astype('category')\n        X_test[CATS] = X_test[CATS].astype('category')\n    \n        Xy_train = IterLoadForDMatrix(Xy_train, FEATURES, 'y')\n        dtrain = xgb.QuantileDMatrix(Xy_train, enable_categorical=True, max_bin=256)\n        dval   = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n        dtest  = xgb.DMatrix(X_test, enable_categorical=True)\n    \n        params['seed'] = kk*FOLDS + fold\n        model = xgb.train(\n            params=params,\n            dtrain=dtrain,\n            num_boost_round=100_000, \n            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n            early_stopping_rounds=400,\n            verbose_eval=500\n        )\n    \n        oof_preds[val_idx] += model.predict(dval, iteration_range=(0, model.best_iteration + 1)) / REPEATS\n        test_preds += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / FOLDS / REPEATS","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":437.088866,"end_time":"2025-08-24T16:34:34.014031","exception":false,"start_time":"2025-08-24T16:27:16.925165","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T06:01:34.462061Z","iopub.execute_input":"2025-08-31T06:01:34.462787Z","iopub.status.idle":"2025-08-31T06:43:02.665299Z","shell.execute_reply.started":"2025-08-31T06:01:34.462761Z","shell.execute_reply":"2025-08-31T06:43:02.664657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nm = roc_auc_score(train.y.to_numpy(), oof_preds)\nprint(f\"XGB (Train More) with Original Data as rows CV = {m}\")","metadata":{"papermill":{"duration":0.43684,"end_time":"2025-08-24T16:34:34.469956","exception":false,"start_time":"2025-08-24T16:34:34.033116","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T06:45:25.176523Z","iopub.execute_input":"2025-08-31T06:45:25.177145Z","iopub.status.idle":"2025-08-31T06:45:25.519076Z","shell.execute_reply.started":"2025-08-31T06:45:25.177120Z","shell.execute_reply":"2025-08-31T06:45:25.518402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train More - Train XGB w/ Original Data as Cols\nPreviously our XGB learning rate was 0.1 for experiments. We will now train it with 0.02 for our final submission. Previously our XGB had 7 folds, we will now train it with 10 folds.","metadata":{"papermill":{"duration":0.016017,"end_time":"2025-08-24T16:34:34.502552","exception":false,"start_time":"2025-08-24T16:34:34.486535","status":"completed"},"tags":[]}},{"cell_type":"code","source":"FEATURES += TE\nprint(f\"We have {len(FEATURES)} features.\")\n\noof_preds2 = np.zeros(len(train))\ntest_preds2 = np.zeros(len(test))\n\nREPEATS = 1\nfor kk in range(REPEATS):\n    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n        print(\"#\"*25)\n        print(f\"### REPEAT {kk+1}, Fold {fold+1} ###\")\n        print(\"#\"*25)\n    \n        Xy_train = train.iloc[train_idx][ FEATURES+['y'] ].copy()  \n        X_valid = train.iloc[val_idx][FEATURES].copy()\n        y_valid = train.iloc[val_idx]['y']\n        X_test = test[FEATURES].copy()\n    \n        CC = CATS1+CATS2\n        print(f\"Target encoding {len(CC)} features... \",end=\"\")\n        for i,c in enumerate(CC):\n            if i%10==0: print(f\"{i}, \",end=\"\")\n            TE0 = TargetEncoder(n_folds=10, smooth=0, split_method='random', stat='mean')\n            Xy_train[c] = TE0.fit_transform(Xy_train[c],Xy_train['y']).astype('float32')\n            X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n            X_test[c] = TE0.transform(X_test[c]).astype('float32')\n        print()\n    \n        Xy_train[CATS] = Xy_train[CATS].astype('category')\n        X_valid[CATS] = X_valid[CATS].astype('category')\n        X_test[CATS] = X_test[CATS].astype('category')\n    \n        Xy_train = IterLoadForDMatrix(Xy_train, FEATURES, 'y')\n        dtrain = xgb.QuantileDMatrix(Xy_train, enable_categorical=True, max_bin=256)\n        dval   = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n        dtest  = xgb.DMatrix(X_test, enable_categorical=True)\n    \n        params['seed'] = kk*FOLDS + fold \n        model = xgb.train(\n            params=params,\n            dtrain=dtrain,\n            num_boost_round=100_000, \n            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n            early_stopping_rounds=250,\n            verbose_eval=500\n        )\n    \n        oof_preds2[val_idx] += model.predict(dval, iteration_range=(0, model.best_iteration + 1)) / REPEATS\n        test_preds2 += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / FOLDS / REPEATS\n\n# FILL NAN FOR NN BELOW (we skipped this above for xgb)\nCC = CATS+CATS1+CATS2\nmn = orig.y.mean()\nfor i,c in enumerate(CC):\n    NAME = f\"TE_ORIG_{c}\"\n    train[NAME] = train[NAME].fillna(mn)\n    test[NAME] = test[NAME].fillna(mn)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":755.207365,"end_time":"2025-08-24T16:47:09.725457","exception":false,"start_time":"2025-08-24T16:34:34.518092","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T06:45:34.470604Z","iopub.execute_input":"2025-08-31T06:45:34.470850Z","iopub.status.idle":"2025-08-31T07:51:08.395082Z","shell.execute_reply.started":"2025-08-31T06:45:34.470834Z","shell.execute_reply":"2025-08-31T07:51:08.394404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nm = roc_auc_score(train.y.to_numpy(), oof_preds2)\nprint(f\"XGB (Train More) with Original Data as rows CV = {m}\")","metadata":{"papermill":{"duration":0.398242,"end_time":"2025-08-24T16:47:10.148024","exception":false,"start_time":"2025-08-24T16:47:09.749782","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:59:50.645148Z","iopub.execute_input":"2025-08-31T07:59:50.645497Z","iopub.status.idle":"2025-08-31T07:59:50.978768Z","shell.execute_reply.started":"2025-08-31T07:59:50.645443Z","shell.execute_reply":"2025-08-31T07:59:50.978113Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Normalize\nNN prefer numerical columns to be Gaussian distributed. Therefore we first apply log transform to skewed distributions and then standardize with subtract mean divide standard deviation. (The standardization code is included in the NN code below).","metadata":{"papermill":{"duration":0.023409,"end_time":"2025-08-24T16:47:10.195693","exception":false,"start_time":"2025-08-24T16:47:10.172284","status":"completed"},"tags":[]}},{"cell_type":"code","source":"LOG = ['balance','duration','campaign','pdays','previous']\n\nfor c in LOG+CE:\n    if c in LOG: \n        mn = min( (train[c].min(), test[c].min()) )\n        train[c] = train[c]-mn\n        test[c] = test[c]-mn\n    train[c] = np.log1p( train[c] )\n    test[c] = np.log1p( test[c] )","metadata":{"papermill":{"duration":0.671415,"end_time":"2025-08-24T16:47:10.891718","exception":false,"start_time":"2025-08-24T16:47:10.220303","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:59:55.895028Z","iopub.execute_input":"2025-08-31T07:59:55.895593Z","iopub.status.idle":"2025-08-31T07:59:56.546153Z","shell.execute_reply.started":"2025-08-31T07:59:55.895567Z","shell.execute_reply":"2025-08-31T07:59:56.545392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = CATS+NUMS+CATS1+CE+TE\nTARGET_COL = 'y'\nprint(f\"We have {len( FEATURES )} features.\")","metadata":{"papermill":{"duration":0.030298,"end_time":"2025-08-24T16:47:10.946846","exception":false,"start_time":"2025-08-24T16:47:10.916548","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:59:58.600731Z","iopub.execute_input":"2025-08-31T07:59:58.601234Z","iopub.status.idle":"2025-08-31T07:59:58.605154Z","shell.execute_reply.started":"2025-08-31T07:59:58.601210Z","shell.execute_reply":"2025-08-31T07:59:58.604567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train More - NN by GPT5 - Vibe Coding!\nI used my features from my XGBoost starter notebook, and then asked GPT5 to build me a NN MLP. Below is what GPT5 produced! Here is the prompt I asked GPT5:\n\n>I have dataframe with 16 categorical features. And i have 279 numeric features. This is in a Pandas dataframe. Can you write me code that builds and trains a Pytorch MLP that uses embeddings for the categoricals and predicts a binary target? Please make the model, set the training schedule. Write a KFold training loop, save the OOF, etc\n\nAfter it produced the code, I tuned the batch size, learning rate, and learning schedule. Everything else was created by GPT5! Wow!\n\nPreviously our NN had 7 folds, we will now train it with 10 folds. And we will train the same 10 folds 5x times. Each time we will use a different seed. Then we will average all the models.","metadata":{"papermill":{"duration":0.023462,"end_time":"2025-08-24T16:47:10.993794","exception":false,"start_time":"2025-08-24T16:47:10.970332","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# =========================\n# PyTorch MLP w/ Cat Embeddings + KFold OOF\n# =========================\nimport os, math, random, gc, json\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n\n# -------------------------\n# Config\n# -------------------------\nSEED          = 42\nFOLDS         = 10\nEPOCHS        = 14\nBATCH_SIZE    = 512\nLR_MAX        = 3e-3          # peak LR for OneCycle\nWD            = 1e-4          # weight decay (AdamW)\nEARLY_STOP    = 4             # epochs with no val AUC improvement\nGRAD_CLIP     = 1.0\nNUM_WORKERS   = 4\nMODEL_DIR     = \"./mlp_catemb_models\"\nOOF_PATH      = \"./oof_catemb.csv\"\n\nos.makedirs(MODEL_DIR, exist_ok=True)\n\nKNOWN_CARDINALITIES = SIZES\ndf = train[ FEATURES+['y'] ].to_pandas()\ndf2 = test[ FEATURES+['y'] ].to_pandas()\n\n# -------------------------\n# Repro\n# -------------------------\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# -------------------------\n# Load/define your dataframe `df`\n# -------------------------\n# df = pd.read_parquet(\"your_data.parquet\")\n# assert TARGET_COL in df.columns\n\ncategorical_cols = list( KNOWN_CARDINALITIES.keys() )\nnumeric_cols = [c for c in df.columns if c not in categorical_cols + [TARGET_COL]]\n\nprint(f\"#categoricals={len(categorical_cols)}, #numerics={len(numeric_cols)}\")\n\n# -------------------------\n# Label-encode categoricals (reserve 0=UNK)\n# -------------------------\nencoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    tmp = pd.concat([ df[[col]],df2[[col]] ],axis=0)\n    tmp[col] = tmp[col].astype(str).fillna(\"NaN\")\n    le.fit(tmp[col].values)\n    df[col] = le.transform(df[col].astype(str).values) + 1\n    df2[col] = le.transform(df2[col].astype(str).values) + 1\n    encoders[col] = le\n    del tmp\n\ncardinalities = {}\nfor col in categorical_cols:\n    if col in KNOWN_CARDINALITIES:\n        cardinalities[col] = KNOWN_CARDINALITIES[col] + 1  # +1 for UNK\n        df[col] = np.clip(df[col], 0, KNOWN_CARDINALITIES[col])\n        df2[col] = np.clip(df2[col], 0, KNOWN_CARDINALITIES[col])\n    else:\n        cardinalities[col] = int( max(df[col].max(),df2[col].max()) ) + 1\n\n# -------------------------\n# Scale numerics\n# -------------------------\nscaler = StandardScaler()\ndf[numeric_cols] = scaler.fit_transform(df[numeric_cols].astype(np.float32))\ndf2[numeric_cols] = scaler.transform(df2[numeric_cols].astype(np.float32))\n\n# -------------------------\n# Embedding dims\n# -------------------------\ndef emb_dim_from_card(n):\n    return int(min(50, round(1.6 * (n**0.56))))\n\nemb_info = [(cardinalities[c], emb_dim_from_card(cardinalities[c])) for c in categorical_cols]\ntotal_emb_dim = sum(d for _, d in emb_info)\nprint(\"Embedding config:\", dict(zip(categorical_cols, emb_info)))\nprint(\"Total embedding dim:\", total_emb_dim, \" + numeric:\", len(numeric_cols))\n\n# -------------------------\n# Dataset\n# -------------------------\nclass TabDataset(Dataset):\n    def __init__(self, df, cat_cols, num_cols, target_col=None, idx=None):\n        self.cat_cols = cat_cols\n        self.num_cols = num_cols\n        self.target_col = target_col\n        self.df = df if idx is None else df.iloc[idx]\n        self.cats = self.df[self.cat_cols].values.astype(np.int64)\n        self.nums = self.df[self.num_cols].values.astype(np.float32)\n        self.y = None if self.target_col is None else self.df[self.target_col].values.astype(np.float32)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n        cats = torch.from_numpy(self.cats[i])\n        nums = torch.from_numpy(self.nums[i])\n        if self.y is None:\n            return cats, nums\n        return cats, nums, torch.tensor(self.y[i])\n\n# -------------------------\n# Model\n# -------------------------\nclass MLPWithCatEmb(nn.Module):\n    def __init__(self, emb_info, n_num, hidden=[512, 256, 128], dropout=0.15):\n        super().__init__()\n        self.emb_layers = nn.ModuleList(\n            [nn.Embedding(num_embeddings=card, embedding_dim=dim, padding_idx=0)\n             for card, dim in emb_info]\n        )\n        emb_total = sum(dim for _, dim in emb_info)\n        in_dim = emb_total + n_num\n\n        self.bn_nums = nn.BatchNorm1d(n_num) if n_num > 0 else nn.Identity()\n        self.dropout = nn.Dropout(dropout)\n\n        layers = []\n        last = in_dim\n        for h in hidden:\n            layers += [\n                nn.Linear(last, h),\n                nn.BatchNorm1d(h),\n                nn.SiLU(),\n                nn.Dropout(dropout)\n            ]\n            last = h\n        self.mlp = nn.Sequential(*layers)\n        self.head = nn.Linear(last, 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x_cat, x_num):\n        emb = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.emb_layers)]\n        x_emb = torch.cat(emb, dim=1) if emb else None\n        if x_num is not None and x_num.shape[1] > 0:\n            x_num = self.bn_nums(x_num)\n            x = torch.cat([x_emb, x_num], dim=1) if x_emb is not None else x_num\n        else:\n            x = x_emb\n        x = self.mlp(x)\n        logit = self.head(x).squeeze(1)\n        return logit\n\n# -------------------------\n# Train / Eval helpers\n# -------------------------\ndef train_one_epoch(model, loader, optimizer, scaler, scheduler=None):\n    model.train()\n    running = 0.0\n    for cats, nums, y in loader:\n        cats = cats.to(device, non_blocking=True)\n        nums = nums.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        with torch.amp.autocast('cuda', enabled=True):\n            logits = model(cats, nums)\n            loss = F.binary_cross_entropy_with_logits(logits, y)\n        scaler.scale(loss).backward()\n        if GRAD_CLIP is not None:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n        scaler.step(optimizer)\n        scaler.update()\n        if scheduler is not None:\n            scheduler.step()\n\n        running += loss.detach().item() * y.size(0)\n    return running / len(loader.dataset)\n\n@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    all_logits, all_y = [], []\n    for batch in loader:\n        if len(batch) == 3:\n            cats, nums, y = batch\n            all_y.append(y.detach().cpu())\n        else:\n            cats, nums = batch\n        cats = cats.to(device, non_blocking=True)\n        nums = nums.to(device, non_blocking=True)\n        all_logits.append(model(cats, nums).detach().cpu())\n\n    logits = torch.cat(all_logits).numpy()\n    probs  = 1.0 / (1.0 + np.exp(-logits))\n    probs_c = np.clip(probs, 1e-7, 1 - 1e-7)  # clip instead of log_loss(..., eps=...)\n\n    if all_y:\n        y_true = torch.cat(all_y).numpy().astype(np.int64)\n        auc = roc_auc_score(y_true, probs)\n        ll  = log_loss(y_true, probs_c)\n        acc = accuracy_score(y_true, (probs >= 0.5).astype(int))\n        return probs, {\"auc\": auc, \"logloss\": ll, \"acc\": acc}\n    return probs, {}\n\noof = np.zeros(len(df), dtype=np.float32)\npreds = np.zeros(len(df2), dtype=np.float32)\n\nREPEATS = 5\nfor kk in range(REPEATS):\n    print(f\"##### REPEAT {kk+1} of {REPEATS} #####\")\n    seed_everything(SEED+kk)\n\n    # -------------------------\n    # KFold training\n    # -------------------------\n    skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n    \n    y = df[TARGET_COL].astype(int).values\n    oof2 = np.zeros(len(df), dtype=np.float32)\n    fold_metrics = []\n    \n    for fold, (trn_idx, val_idx) in enumerate(skf.split(df, y), start=1):\n        print(f\"\\n========== Fold {fold}/{FOLDS} ==========\")\n        trn_ds = TabDataset(df, categorical_cols, numeric_cols, TARGET_COL, trn_idx)\n        val_ds = TabDataset(df, categorical_cols, numeric_cols, TARGET_COL, val_idx)\n        test_ds = TabDataset(df2, categorical_cols, numeric_cols, TARGET_COL, np.arange(len(df2)) )\n    \n        trn_loader = DataLoader(\n            trn_ds, batch_size=BATCH_SIZE, shuffle=True,\n            num_workers=NUM_WORKERS, pin_memory=True, drop_last=True\n        )\n        val_loader = DataLoader(\n            val_ds, batch_size=BATCH_SIZE, shuffle=False,\n            num_workers=NUM_WORKERS, pin_memory=True\n        )\n        test_loader = DataLoader(\n            test_ds, batch_size=BATCH_SIZE, shuffle=False,\n            num_workers=NUM_WORKERS, pin_memory=True\n        )\n    \n        model = MLPWithCatEmb(emb_info=emb_info, n_num=len(numeric_cols)).to(device)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=LR_MAX, weight_decay=WD)\n    \n        total_steps = EPOCHS * len(trn_loader)\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=LR_MAX, total_steps=total_steps,\n            pct_start=0.0, div_factor=25.0, final_div_factor=10.5, anneal_strategy='cos'\n        )\n    \n        scaler = torch.amp.GradScaler('cuda', enabled=True)\n    \n        best_auc = -1.0\n        best_epoch = -1\n        epochs_no_improve = 0\n        best_path = os.path.join(MODEL_DIR, f\"fold{fold}.pt\")\n    \n        for epoch in range(1, EPOCHS+1):\n            train_loss = train_one_epoch(model, trn_loader, optimizer, scaler, scheduler)\n            _, val_stats = validate(model, val_loader)\n            auc = val_stats.get(\"auc\", float(\"nan\"))\n            print(f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} | \"\n                  f\"val_auc={auc:.5f} val_logloss={val_stats['logloss']:.5f} val_acc={val_stats['acc']:.4f}\")\n    \n            if 1: #auc > best_auc:\n                best_auc = auc\n                best_epoch = epoch\n                epochs_no_improve = 0\n                torch.save({\n                    \"model_state\": model.state_dict(),\n                    \"config\": {\n                        \"emb_info\": emb_info,\n                        \"numeric_cols\": numeric_cols,\n                        \"categorical_cols\": categorical_cols\n                    }\n                }, best_path)\n            else:\n                epochs_no_improve += 1\n                if epochs_no_improve >= EARLY_STOP:\n                    print(f\"Early stopping at epoch {epoch}. Best AUC {best_auc:.5f} @ epoch {best_epoch}\")\n                    break\n    \n        # Load best\n        ckpt = torch.load(best_path, map_location=\"cpu\", weights_only=False)\n        model.load_state_dict(ckpt[\"model_state\"])\n        model.to(device)\n    \n        # OOF for this fold\n        val_probs, val_stats = validate(model, val_loader)\n        oof[val_idx] += val_probs / REPEATS\n        oof2[val_idx] = val_probs\n        fold_metrics.append({\"fold\": fold, **val_stats})\n        print(f\"[Fold {fold}] AUC={val_stats['auc']:.5f}  LogLoss={val_stats['logloss']:.5f}  Acc={val_stats['acc']:.4f}\")\n    \n        test_probs, _ = validate(model, test_loader)\n        preds += test_probs / FOLDS / REPEATS\n    \n    # -------------------------\n    # Overall OOF metrics\n    # -------------------------\n    oof_c = np.clip(oof2, 1e-7, 1 - 1e-7)\n    oof_auc = roc_auc_score(y, oof2)\n    oof_ll  = log_loss(y, oof_c)     # no eps kwarg\n    oof_acc = accuracy_score(y, (oof2>=0.5).astype(int))\n    print(\"\\n========== Overall OOF ==========\")\n    print(f\"OOF AUC={oof_auc:.5f}  LogLoss={oof_ll:.5f}  Acc={oof_acc:.4f}\")\n    \n    # Save OOF and metrics\n    #pd.DataFrame({\n    #    \"oof_pred\": oof,\n    #    TARGET_COL: y\n    #}).to_csv(OOF_PATH, index=False)\n    \n    #with open(os.path.join(MODEL_DIR, \"fold_metrics.json\"), \"w\") as f:\n    #    json.dump({\"folds\": fold_metrics, \"oof\": {\"auc\": oof_auc, \"logloss\": oof_ll, \"acc\": oof_acc}}, f, indent=2)\n    \n    #print(f\"Saved OOF -> {OOF_PATH}\")\n    #print(f\"Saved models -> {MODEL_DIR}\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":2218.617881,"end_time":"2025-08-24T17:24:09.635138","exception":false,"start_time":"2025-08-24T16:47:11.017257","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:00:17.706981Z","iopub.execute_input":"2025-08-31T08:00:17.707235Z","iopub.status.idle":"2025-08-31T08:54:22.285350Z","shell.execute_reply.started":"2025-08-31T08:00:17.707216Z","shell.execute_reply":"2025-08-31T08:54:22.284508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"m = roc_auc_score(train.y.to_numpy(), oof)\nprint(f\"NN (Train More) CV = {m}\")","metadata":{"papermill":{"duration":0.408861,"end_time":"2025-08-24T17:24:10.076582","exception":false,"start_time":"2025-08-24T17:24:09.667721","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:54:59.430824Z","iopub.execute_input":"2025-08-31T08:54:59.431833Z","iopub.status.idle":"2025-08-31T08:54:59.757407Z","shell.execute_reply.started":"2025-08-31T08:54:59.431799Z","shell.execute_reply":"2025-08-31T08:54:59.756773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create NN (Train More) Submission CSV","metadata":{"papermill":{"duration":0.034598,"end_time":"2025-08-24T17:24:10.146248","exception":false,"start_time":"2025-08-24T17:24:10.11165","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub = pd.read_csv(f\"{PATH}sample_submission.csv\")\nsub['y'] = preds\nsub.to_csv(\"submission_nn_train_more.csv\",index=False)\nnp.save(\"oof_nn_train_more\",oof)\nprint('Submission shape',sub.shape)\n#sub.head()","metadata":{"papermill":{"duration":0.539812,"end_time":"2025-08-24T17:24:10.720597","exception":false,"start_time":"2025-08-24T17:24:10.180785","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:55:07.595370Z","iopub.execute_input":"2025-08-31T08:55:07.595672Z","iopub.status.idle":"2025-08-31T08:55:08.090338Z","shell.execute_reply.started":"2025-08-31T08:55:07.595651Z","shell.execute_reply":"2025-08-31T08:55:08.089726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create XGB (Train More) Submission CSV","metadata":{"papermill":{"duration":0.031598,"end_time":"2025-08-24T17:24:12.54747","exception":false,"start_time":"2025-08-24T17:24:12.515872","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub = pd.read_csv(f\"{PATH}sample_submission.csv\")\npreds_xgb = (test_preds+test_preds2)/2. \nsub['y'] = preds_xgb\nsub.to_csv(\"submission_xgb_train_more.csv\",index=False)\nnp.save(\"oof_xgb_rows_train_more\",oof_preds)\nnp.save(\"oof_xgb_cols_train_more\",oof_preds2)\nprint('Submission shape',sub.shape)\n#sub.head()","metadata":{"papermill":{"duration":0.638947,"end_time":"2025-08-24T17:24:13.218726","exception":false,"start_time":"2025-08-24T17:24:12.579779","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:56:45.768892Z","iopub.execute_input":"2025-08-31T08:56:45.769499Z","iopub.status.idle":"2025-08-31T08:56:46.323485Z","shell.execute_reply.started":"2025-08-31T08:56:45.769474Z","shell.execute_reply":"2025-08-31T08:56:46.322701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble - XGB and NN (Train More) - CV Score","metadata":{}},{"cell_type":"code","source":"oof_xgb = (oof_preds+oof_preds2)/2.\nm = roc_auc_score(train.y.to_numpy(), oof_xgb)\nprint(f\"Both XGB rows and XGB cols (Train More) CV = {m}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:58:36.848682Z","iopub.execute_input":"2025-08-31T08:58:36.849321Z","iopub.status.idle":"2025-08-31T08:58:37.198902Z","shell.execute_reply.started":"2025-08-31T08:58:36.849295Z","shell.execute_reply":"2025-08-31T08:58:37.198035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_m = 0\nbest_w = 0\nfor w in np.arange(0,1.01,0.01):\n    oof_ensemble = (1-w)*oof_xgb + w*oof\n    m = roc_auc_score(train.y.to_numpy(), oof_ensemble)\n    if m>best_m:\n        best_m = m\n        best_w = w\n        \noof_ensemble = (1-best_w)*oof_xgb + best_w*oof\nm = roc_auc_score(train.y.to_numpy(), oof_ensemble)\nprint(f\"Ensemble XGB and NN (Train More) CV = {m}\")\nprint(f\" using best NN weight = {best_w}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:58:41.088240Z","iopub.execute_input":"2025-08-31T08:58:41.088601Z","iopub.status.idle":"2025-08-31T08:59:15.322394Z","shell.execute_reply.started":"2025-08-31T08:58:41.088575Z","shell.execute_reply":"2025-08-31T08:59:15.321568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create - XGB and NN (Train More) - Submission CSV","metadata":{"papermill":{"duration":0.034203,"end_time":"2025-08-24T17:24:54.741585","exception":false,"start_time":"2025-08-24T17:24:54.707382","status":"completed"},"tags":[]}},{"cell_type":"code","source":"xgb_preds = preds_xgb \nsub = pd.read_csv(f\"{PATH}sample_submission.csv\")\nsub['y'] = (1-best_w)*xgb_preds + best_w*preds\nsub.to_csv(\"submission_ensemble_train_more.csv\",index=False)\nprint('Submission shape',sub.shape)\nsub.head()","metadata":{"papermill":{"duration":0.660638,"end_time":"2025-08-24T17:24:55.438177","exception":false,"start_time":"2025-08-24T17:24:54.777539","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:59:36.557871Z","iopub.execute_input":"2025-08-31T08:59:36.558559Z","iopub.status.idle":"2025-08-31T08:59:37.102762Z","shell.execute_reply.started":"2025-08-31T08:59:36.558532Z","shell.execute_reply":"2025-08-31T08:59:37.102071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}